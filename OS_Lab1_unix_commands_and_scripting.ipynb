{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Unix commands and scripting\n",
    "\n",
    "\n",
    "There are several great resources to get you started on Unix commands. Nemeth is a good reference to get started. \n",
    "\n",
    "- Nemeth, E.; Snyder, G.; Hein, T. R. & Whaley, B. Taub, M. (Ed.) Unix and Linux System Administration Handbook Prentice Hall, 2010. **Required reading: chapter 2, pp 29-72**\n",
    "- Kerrisk, M. The Linux Programming Interface: A Linux and UNIX System Programming Handbook No Starch Press, 2010.\n",
    "- Blum, R. & Bresnahan, C. Linux Command Line and Shell Scripting Bible John Wiley &38; Sons, Inc., 2015.\n",
    "- Ohad Rodeh, Josef Bacik, and Chris Mason. 2013. BTRFS: The Linux B-Tree Filesystem. Trans. Storage 9, 3, Article 9 (August 2013), 32 pages\n",
    "\n",
    "You can practice linux commands online at [hackerrank](https://www.hackerrank.com/domains/shell) and [learnshell](http://www.learnshell.org/). More complex based bash tools can be obtained at [ostechnix](https://www.ostechnix.com/collection-useful-bash-scripts-heavy-commandline-users/) and [awesome-bash](https://github.com/awesome-lists/awesome-bash).\n",
    "\n",
    "Two assignments are given. Assignment 1 request you build pipeline commands. Assignment 2 requires you build bash and Python scripts.\n",
    "\n",
    "- Assigment date: **August 23, 2019**\n",
    "- Duedate: **August 30, 2019. 4:00 PM**\n",
    "- Students must work in teams of two. Teams must apply [pair programming](https://en.wikipedia.org/wiki/Pair_programming). \n",
    "- Submit to git: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic linux commands\n",
    "\n",
    "A prerequisite for this notebook is that you have knowledge on the usage of the Linux desktop environment and on the usage of basic command line commands. See introduction to Linux for an introductory lecture (optional). \n",
    "\n",
    "**Note. You must read reference [1] to continue.** Linux command are usually run in terminal. To facilitate explanations, we will run them in Jupiter. However, you will be eventually required to work in a text terminal.\n",
    "\n",
    "Jupyter interprets code in cells. By default, all cells in a notebook interprets code in the language selected at the notebook creation. For instance, Python. Cells, however, can interpret other languages by defining [cell magics]( https://ipython.readthedocs.io/en/stable/interactive/magics.html#cell-magics).  For instance, %%bash, %%html, %%perl, and many others. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ahiralesc/mysrc/notebooks/sistemas_operativos\n",
      "OS_Conferences_and_resources.ipynb\n",
      "OS_Lab1_unix_commands_and_scripting.ipynb\n",
      "archivo.txt\n",
      "OS_Conferences_and_resources.ipynb\n",
      "OS_Lab1_unix_commands_and_scripting.ipynb\n"
     ]
    }
   ],
   "source": [
    "%%bash \n",
    "# Let’s begin by running commands that retrieve state from the file system or alter it\n",
    "\n",
    "# Gets the current working directory\n",
    "pwd\n",
    "\n",
    "# Lists files and directories stored in the current working directory\n",
    "ls\n",
    "\n",
    "# Creates, a temporary directory named tmp \n",
    "mkdir tmp\n",
    "\n",
    "# Changes the location of the working directory to tmp\n",
    "cd tmp\n",
    "\n",
    "# Creates an empty file\n",
    "touch archivo.txt\n",
    "\n",
    "# Again, lists the contents of the current working directory\n",
    "ls \n",
    "\n",
    "# Remioves the file archivo.txt a single file\n",
    "rm archivo.txt\n",
    "\n",
    "# Changes location to the parent directory\n",
    "cd ..\n",
    "\n",
    "# Deletes the subdirectory tmp\n",
    "rm -rf tmp\n",
    "\n",
    "# Again, lists the contents of the current working directory\n",
    "ls "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Files and directories can be accessed by tree class of users: user –the owner-, group, and everyone. Properties can be viewed via the ls comand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 6260\r\n",
      "drwxr-xr-x 23 jonathan jonathan    4096 ago 28 13:08 \u001b[0m\u001b[01;34m.\u001b[0m/\r\n",
      "drwxr-xr-x  3 root     root        4096 oct  9  2017 \u001b[01;34m..\u001b[0m/\r\n",
      "drwx------  3 jonathan jonathan    4096 oct 27  2017 \u001b[01;34m.adobe\u001b[0m/\r\n",
      "drwxr-xr-x 24 jonathan jonathan    4096 ago 28 12:58 \u001b[01;34manaconda\u001b[0m/\r\n",
      "drwxr-xr-x  3 jonathan jonathan    4096 ago 28 13:01 \u001b[01;34m.anaconda\u001b[0m/\r\n",
      "-rw-------  1 jonathan jonathan    1296 ago 28 12:03 .bash_history\r\n",
      "-rw-r--r--  1 jonathan jonathan     220 oct  9  2017 .bash_logout\r\n",
      "-rw-r--r--  1 jonathan jonathan    3806 ago 28 12:56 .bashrc\r\n",
      "drwx------ 21 jonathan jonathan    4096 ago 28 10:06 \u001b[01;34m.cache\u001b[0m/\r\n",
      "drwx------  3 jonathan jonathan    4096 oct  9  2017 \u001b[01;34m.compiz\u001b[0m/\r\n",
      "drwxr-xr-x  3 jonathan jonathan    4096 ago 28 13:08 \u001b[01;34m.conda\u001b[0m/\r\n",
      "-rw-r--r--  1 jonathan jonathan      40 ago 28 13:02 .condarc\r\n",
      "drwx------ 21 jonathan jonathan    4096 ago 28 09:13 \u001b[01;34m.config\u001b[0m/\r\n",
      "drwx------  3 root     root        4096 ago 28 10:10 \u001b[01;34m.dbus\u001b[0m/\r\n",
      "drwxr-xr-x  3 jonathan jonathan    4096 jul  5  2018 \u001b[01;34mDesktop\u001b[0m/\r\n",
      "drwxr-xr-x  2 jonathan jonathan    4096 oct  9  2017 \u001b[01;34mDocuments\u001b[0m/\r\n",
      "drwxr-xr-x  2 jonathan jonathan    4096 ago 28 13:05 \u001b[01;34mDownloads\u001b[0m/\r\n",
      "-rw-r--r--  1 jonathan jonathan    8980 oct  9  2017 examples.desktop\r\n",
      "-rw-------  1 jonathan jonathan   22298 ago 28 12:03 .ICEauthority\r\n",
      "drwxr-xr-x  2 jonathan jonathan    4096 ago 28 13:05 \u001b[01;34m.ipynb_checkpoints\u001b[0m/\r\n",
      "drwxr-xr-x  5 jonathan jonathan    4096 ago 28 13:06 \u001b[01;34m.ipython\u001b[0m/\r\n",
      "drwx------  3 jonathan jonathan    4096 oct  9  2017 \u001b[01;34m.local\u001b[0m/\r\n",
      "drwx------  3 jonathan jonathan    4096 oct 27  2017 \u001b[01;34m.macromedia\u001b[0m/\r\n",
      "drwx------  5 jonathan jonathan    4096 jul  5  2018 \u001b[01;34m.mozilla\u001b[0m/\r\n",
      "drwxr-xr-x  2 jonathan jonathan    4096 oct  9  2017 \u001b[01;34mMusic\u001b[0m/\r\n",
      "-rw-r--r--  1 jonathan jonathan    4763 ago 28 13:05 OS_Lab0_conferences_and_resources.ipynb\r\n",
      "-rw-r--r--  1 jonathan jonathan   24421 ago 28 13:08 OS_Lab1_unix_commands_and_scripting.ipynb\r\n",
      "-rw-r--r--  1 jonathan jonathan   14666 ago 28 13:05 OS_Lab4_build_automation.ipynb\r\n",
      "-rw-r--r--  1 jonathan jonathan 1335641 ago 28 13:05 \u001b[01;31mpart-00003-of-00500.csv.gz\u001b[0m\r\n",
      "-rw-r--r--  1 jonathan jonathan 1798429 ago 28 13:05 \u001b[01;31mpart-00006-of-00500.csv.gz\u001b[0m\r\n",
      "-rw-r--r--  1 jonathan jonathan 3044219 ago 28 13:05 \u001b[01;31mpart-00009-of-00500.csv.gz\u001b[0m\r\n",
      "drwxr-xr-x  2 jonathan jonathan    4096 oct  9  2017 \u001b[01;34mPictures\u001b[0m/\r\n",
      "-rw-r--r--  1 jonathan jonathan     675 oct  9  2017 .profile\r\n",
      "drwxr-xr-x  2 jonathan jonathan    4096 oct  9  2017 \u001b[01;34mPublic\u001b[0m/\r\n",
      "-rw-------  1 jonathan jonathan      26 ago 28 13:01 .python_history\r\n",
      "-rw-r--r--  1 jonathan jonathan       0 ago 27 16:26 .sudo_as_admin_successful\r\n",
      "drwxr-xr-x  2 jonathan jonathan    4096 oct  9  2017 \u001b[01;34mTemplates\u001b[0m/\r\n",
      "drwxr-xr-x  2 jonathan jonathan    4096 oct  9  2017 \u001b[01;34mVideos\u001b[0m/\r\n",
      "-rw-------  1 jonathan jonathan      60 ago 28 12:03 .Xauthority\r\n",
      "-rw-------  1 jonathan jonathan    3708 ago 28 12:03 .xsession-errors\r\n",
      "-rw-------  1 jonathan jonathan    3708 ago 28 11:08 .xsession-errors.old\r\n"
     ]
    }
   ],
   "source": [
    "ls -al"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See [ls -l field explanation](http://blmrgnn.blogspot.com/2016/11/ls-l-field-explanation.html) and [Linux and Unix ls command tutorial with examples](https://shapeshed.com/unix-ls/) for field properties and other use cases.\n",
    "\n",
    "Internally, Linux organizes files and directories in a B-tree [4]. Logically, files and directories are organized in a tree-like data structure. Mayor, or level one, directories include: bin, boot, dev, etc. See the [linux file system explained](https://www.linux.com/blog/learn/intro-to-linux/2018/4/linux-filesystem-explained) for details. You can list level one subdirectories with the *tree* command. i.e. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bash: line 1: tree: command not found\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "tree -L 1 /"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bash: line 2: tree: command not found\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# Your $HOME directory is \n",
    "tree -L 1 $HOME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some many applications for bash. In what fallows, we will focus in using bash as a [scripting language](https://en.wikipedia.org/wiki/Scripting_language) for processing large data files. A data file is stored either in binary or text format. Data in it, is organized further in human, or non-human, readable formats. Examples of human readable formats include: [CVS](https://en.wikipedia.org/wiki/Comma-separated_values), [XML](https://en.wikipedia.org/wiki/XML), and [JSON](https://en.wikipedia.org/wiki/JSON). \n",
    "\n",
    "The aim of the following scripts is to estimate statistical data from a set of log files. We will assume, the log files are store in a subdirectory named logs. You must download tree files from blackboard contenido/logs subdirectory and store them in your logs subdirectory. **You must do the later manually**. Logs are CSV formatted. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# Set the name of the working directory\n",
    "working_dir=logs # Do not include black spaces.\n",
    "\n",
    "# If the logs subdirectory does not exist, create it.\n",
    "if [ ! -f $working_dir ]; # ! stands for not or negation\n",
    "then\n",
    "    mkdir $working_dir # Creates the subdirectory logs\n",
    "fi\n",
    "# Copy the logs from blackboard to the previos subdirectory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Files are labeled *part-00000-of-00500.csv.gz, part-00001-of-00500.csv.gz, and part-00003-of-00500.csv.gz*. These are very large CSV files from the [google cluster repository](https://github.com/google/cluster-data). First lets assume you don't know how many files are in the logs subdirectory and you want to find out such information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%bash --out log_file_names --err error\n",
    "# Note. A little trick is applied so that STDOUT is redirected to your machine and stored in log_file_names\n",
    "ls -AS logs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "part-00000-of-00500.csv.gz\n",
      "part-00002-of-00500.csv.gz\n",
      "part-00001-of-00500.csv.gz\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# The following code is Python based.\n",
    "print(log_file_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each line in a log corresponds to a task event. There are 13 rows with the following information. \n",
    "\n",
    "- timestamp\n",
    "- missing_info,\n",
    "- job_id,\n",
    "- task_index,\n",
    "- machine_id,\n",
    "- event_type,\n",
    "- user_name,\n",
    "- scheduling_class,\n",
    "- priority,\n",
    "- rr_cpu,\n",
    "- rr_ram,\n",
    "- rr_disk,\n",
    "- constraints \n",
    "\n",
    "Lets assume you need a script that counts the number of events in each log. [Parameter expantion](http://wiki.bash-hackers.org/syntax/pe) is applied in the following example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "part-00000-of-00500.csv.gz, 450146\n",
      "part-00002-of-00500.csv.gz, 160642\n",
      "part-00001-of-00500.csv.gz, 77776\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# In a bash, the first line of a script header contains the line\n",
    "# !/bin/bash followed by linux command lines\n",
    "\n",
    "#  We will count the number of events in each file and store individual counts in a text file\n",
    "\n",
    "# First, lets make sure the file is not present. If so, delete it.\n",
    "if [ -f events_per_log.txt ]; then\n",
    "    rm events_per_log.txt\n",
    "fi\n",
    "\n",
    "# Parameter expantion is applied to get all file names in a given path location\n",
    "# The path and file name is stored in variable i.\n",
    "# The zcat uncompresses file i and fowards the output to the pipe |\n",
    "# The pipe fowards its output to the wc \n",
    "# finally, wc counts the number of lines per file and assings such count to the\n",
    "# variable lines\n",
    "# The echo command creates a formatted string and appends (>>) the output to the\n",
    "# file name events_per_log.txt\n",
    "# NOTE: YOU MUST CHANGE THE PATH LOCATION\n",
    "for i in $(ls -AS ~ahiralesc/mysrc/notebooks/sistemas_operativos/logs/); do\n",
    "    lines=$(zcat ~ahiralesc/mysrc/notebooks/sistemas_operativos/logs/\"$i\" | wc -l)\n",
    "    echo -e \"$i, $lines\" >> events_per_log.txt\n",
    "done\n",
    "\n",
    "# Head is used to show the begining lines of the file events_per_log.txt\n",
    "head events_per_log.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assigment 1.\n",
    "\n",
    "Your objective will be to build the following table using several bash scripts. \n",
    "\n",
    "|  Log file properties |      |             |                       |                       |\n",
    "|:--------------------:|------|-------------|-----------------------|-----------------------|\n",
    "| Name                 | Size | Num. Events | Start time            | End time              |\n",
    "| part-00000-of-00500  | 100  | 20          | 2011-05-01 19:00:00.000000 | 2011-05-01 19:00:00.000000 |\n",
    "| part-00001-of-00500  | 102  | 30          | 2011-05-01 19:00:00.000000 | 2011-05-01 19:00:00.000000 |\n",
    "| part-00002-of-00500  | 102  | 30          | 2011-05-01 19:00:00.000000 | 2011-05-01 19:00:00.000000 |\n",
    "\n",
    "Note you must produce a CSV file, not a visual table. Thus the final output will be a file with the following content.\n",
    "\n",
    "```bash\n",
    "Name, Size, Num. Events, Start time, End time\n",
    "part-00000-of-00500, 100, 20, 2011-05-01 19:00:00.000000, 2011-05-01 19:00:00.000000\n",
    "part-00001-of-00500, 102, 30, 2011-05-01 19:00:00.000000, 2011-05-01 19:00:00.000000\n",
    "part-00002-of-00500, 102, 30, 2011-05-01 19:00:00.000000, 2011-05-01 19:00:00.000000\n",
    "```\n",
    "**You must submit two products**:\n",
    "- A single bash script that produces the previous output.\n",
    "- The CSV file produced by the bash script.\n",
    "\n",
    "In what follows, I will give some hints on how to approach the problem. \n",
    "\n",
    "#### Extracting the file name\n",
    "\n",
    "Consider the filename part-00000-of-00500.csv.gz. The first task to address is to extract the string part-00000-of-00500."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "path=/home/ahiralesc/mysrc/cutu/extraction/workload\n",
      "path=/home/ahiralesc/mysrc/cutu/extraction/workload\n",
      "filename=part-00000-of-00500.csv.gz\n",
      "filename=part-00000-of-00500.csv.gz\n",
      "pref=part-00000-of-00500.csv\n",
      "ext=gz\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# Assume the file part-00000-of-00500.csv.gz is located in /home/ahiralesc/mysrc/cutu/extraction/workload/\n",
    "file=/home/ahiralesc/mysrc/cutu/extraction/workload/part-00000-of-00500.csv.gz\n",
    "# The following code extracts the filename\n",
    "xpath=${file%/*}        # Gets the file path: /home/ahiralesc/mysrc/cutu/extraction/workload/\n",
    "xbase=${file##*/}       # Gets the file name: part-00000-of-00500.csv\n",
    "xfext=${xbase##*.}      # Gets the extension: csv\n",
    "xpref=${xbase%.*}       # Truncates the file extension: part-00000-of-00500\n",
    "\n",
    "echo;\n",
    "echo path=${xpath};\n",
    "echo path=$(dirname $file)         # Here is another way to get the path\n",
    "echo filename=${xbase};\n",
    "echo filename=$(basename $file)    # Here is another way to get the filename\n",
    "echo pref=${xpref};\n",
    "echo ext=${xfext}\n",
    "echo;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A real bash script is often stored in a file and has sh extension. For instance, get_filenames.sh In it, the previous code can be structured as fallows:\n",
    "```bash\n",
    "#!/bin/bash\n",
    "xpath=${1%/*}        # Gets the file path: /home/ahiralesc/mysrc/cutu/extraction/workload/\n",
    "xbase=${1##*/}       # Gets the file name: part-00000-of-00500.csv\n",
    "xfext=${xbase##*.}      # Gets the extension: csv\n",
    "xpref=${xbase%.*}       # Truncates the file extension: part-00000-of-00500\n",
    "\n",
    "echo;\n",
    "echo path=${xpath};\n",
    "echo path=$(dirname $1)         # Here is another way to get the path\n",
    "echo filename=${xbase};\n",
    "echo filename=$(basename $1)    # Here is another way to get the filename\n",
    "echo pref=${xpref};\n",
    "echo ext=${xfext}\n",
    "echo;\n",
    "```\n",
    "\n",
    "Not the label **file** has been replaced with **1**. This corresponds to the first argument given to your script. For instance,\n",
    "\n",
    "```bash\n",
    "./get_filenames.sh part-00000-of-00500.csv.gz\n",
    "```\n",
    "\n",
    "\n",
    "Before, trying to execute the previous command in a shell. You must grant the script execution privileges. This is done with the chmod +x command. \n",
    "\n",
    "```bash\n",
    "chmod +x part-00000-of-00500.csv.gz\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assigment 1.1. [ 25 pts.]\n",
    "\n",
    "Create a script that given a path to the *gz files reads each file and extracts the filenames.\n",
    "\n",
    "#### Getting the size of the file name\n",
    "\n",
    "File information is already available via the ls command. The fifth column of ls -la gives just what we need. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 6728\r\n",
      "drwxrwxr-x 2 ahiralesc ahiralesc    4096 Aug 13 13:15 \u001b[0m\u001b[01;34m.\u001b[0m/\r\n",
      "drwxrwxr-x 4 ahiralesc ahiralesc    4096 Aug 14 12:52 \u001b[01;34m..\u001b[0m/\r\n",
      "-rw-rw-r-- 1 ahiralesc ahiralesc 4128899 Aug 13 13:10 \u001b[01;31mpart-00000-of-00500.csv.gz\u001b[0m\r\n",
      "-rw-rw-r-- 1 ahiralesc ahiralesc  924634 Aug 13 13:10 \u001b[01;31mpart-00001-of-00500.csv.gz\u001b[0m\r\n",
      "-rw-rw-r-- 1 ahiralesc ahiralesc 1821031 Aug 13 13:10 \u001b[01;31mpart-00002-of-00500.csv.gz\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "ls -al logs/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, there is a lot of clutter. We don’t  need directory information just filenames. You must find the proper ls switches that disable printing of directory information. The following code, uses [awk](https://www.tutorialspoint.com/awk/), a text processing command to extract column two of ls output. Awk reads stdin in $0. It later uses split to partition the input string in an array labeled a. Such is latter indexed to extract the field of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6728\r\n",
      "2\r\n",
      "4\r\n",
      "1\r\n",
      "1\r\n",
      "1\r\n"
     ]
    }
   ],
   "source": [
    "ls -al logs/  | awk '{split($0,a); print a[2];}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assigment 1.2 [ 25 pts.]\n",
    "\n",
    "Create a script that given a path to the *gz files estimates the size of each log file. \n",
    "\n",
    "#### working with time\n",
    "\n",
    "The first trace, part-00000-of-00500.csv.gz, starts at 19:00 Hrs. EDT on Sunday May 1, 2011, and the datacenter is in that timezone (US Eastern/daylight saving time). Timestamps begin 600 seconds before the beginning of the trace period. The instance of time at wich an event ocurred is stored in the variable timestamp. However, all the first event in log part-00000-of-00500.csv.gz occurs at timestamp 0. This is wrong! Furthermore, timestamps are stored in microseconds, not milliseconds. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first log time stamp starts at (milliseconds) :  1304301600000\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "from pytz import timezone\n",
    "from time import mktime\n",
    "\n",
    "# EDT is equivalent to US/Eastern + daytime savings\n",
    "eastern = timezone('US/Eastern')\n",
    "time = eastern.localize(datetime(2011, 5, 1, 19, 0, 0, 0), is_dst=True).timetuple()\n",
    "time = int(mktime(time) * 1000)\n",
    "print('The first log time stamp starts at (milliseconds) : ', time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You must add this time (1304301600000) to the timestamps of all events in all logs. How do we do that? Consider the following code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "gzip: logs/part-00003-of-00500.csv.gz: No such file or directory\n",
      "gzip: logs/part-00003-of-00500.csv.gz: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "zcat logs/part-00000-of-00500.csv.gz | head -n 1 | awk -F \"\\\"*,\\\"*\" '{ORS=\"\\t\"} {split($0,a); printf \"%.2f\",((a[1] * 0.001)+ 1304301600000);}'\n",
    "echo ;\n",
    "zcat logs/part-00000-of-00500.csv.gz | tail -n 1 | awk -F \"\\\"*,\\\"*\" '{ORS=\"\\t\"} {split($0,a); printf \"%.2f\",((a[1] * 0.001)+ 1304301600000);}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now timestamps are correct. The output must be redirected to file. Then, you must extract the first and last timestamp and transform these values to human readable format with the previous Python code. For instance, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2011-05-01 19:00:00.000000'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datetime.fromtimestamp(float(1304301600000)/1000).strftime('%Y-%m-%d %H:%M:%S.%f')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assigment 1.3 [ 25 pts.]\n",
    "\n",
    "Create a script that computes each log start and end times.\n",
    "\n",
    "### Assigment 1.4 [ 25 pts.]\n",
    "\n",
    "Finally, create a script that invoke scrips 1 to 3 and produces the requested CSV file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "part-00000-of-00500.csv.gz 450146\n",
      "part-00002-of-00500.csv.gz 160642\n",
      "part-00001-of-00500.csv.gz 77776\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# Solution 1.1. Extrating the log name and estimating the number of events per log\n",
    "if [ -f events_per_log.txt ]; then\n",
    "    rm events_per_log.txt\n",
    "fi\n",
    "\n",
    "for i in $(ls -AS ~ahiralesc/mysrc/notebooks/sistemas_operativos/logs/); do\n",
    "    lines=$(zcat ~ahiralesc/mysrc/notebooks/sistemas_operativos/logs/\"$i\" | wc -l)\n",
    "    xbase=${i##*/}\n",
    "    echo -e \"$xbase $lines\" >> events_per_log.txt\n",
    "done\n",
    "\n",
    "cat events_per_log.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd Documents\n",
    "\n",
    "\n",
    "if [ -f Logs/logs1 ]; then\n",
    "    rm Logs/logs1\n",
    "fi\n",
    "\n",
    "echo \"NOMBRE\" >> Logs/logs1\n",
    "\n",
    "for i in $(ls -l SistemasOperativos/ | awk '{split($0,a); print a[9]}');\n",
    "\n",
    "do\n",
    "    xbase=${i##*/}\n",
    "    xpref=${xbase%%.*}\n",
    "    echo ${xpref} >> Logs/logs1\n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "*last update: August 23, 2019*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%bash \n",
    "cd Documents\n",
    "\n",
    "\n",
    "if [ -f Logs/logs2 ]; then\n",
    "    rm Logs/logs2\n",
    "fi\n",
    "\n",
    "echo \"TAMANO\" >> Logs/logs2\n",
    "\n",
    "for i in $(ls -l  SistemasOperativos/ | awk '{split($0,a); print a[5];}')\n",
    "do\n",
    "   echo ${i} >> Logs/logs2\n",
    "done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1304317234546.29\n",
      "1304322244631.19\n",
      "1304332267297.97\n",
      "1304337277640.49\n",
      "1304347300877.30\n",
      "1304352311946.04\n"
     ]
    }
   ],
   "source": [
    "%%bash \n",
    "\n",
    "cd Documents/SistemasOperativos\n",
    "\n",
    "zcat part-00003-of-00500.csv.gz | head -n 1 | awk -F \"\\\"*,\\\"*\" '{ORS=\"\\t\"} {split($0,a); printf \"%.2f\",((a[1] * 0.001)+ 1304301600000);}'\n",
    "echo ;\n",
    "zcat part-00003-of-00500.csv.gz | tail -n 1 | awk -F \"\\\"*,\\\"*\" '{ORS=\"\\t\"} {split($0,a); printf \"%.2f\",((a[1] * 0.001)+ 1304301600000);}'\n",
    "echo ;\n",
    "zcat part-00006-of-00500.csv.gz | head -n 1 | awk -F \"\\\"*,\\\"*\" '{ORS=\"\\t\"} {split($0,a); printf \"%.2f\",((a[1] * 0.001)+ 1304301600000);}'\n",
    "echo ;\n",
    "zcat part-00006-of-00500.csv.gz | tail -n 1 | awk -F \"\\\"*,\\\"*\" '{ORS=\"\\t\"} {split($0,a); printf \"%.2f\",((a[1] * 0.001)+ 1304301600000);}'\n",
    "echo ;\n",
    "zcat part-00009-of-00500.csv.gz | head -n 1 | awk -F \"\\\"*,\\\"*\" '{ORS=\"\\t\"} {split($0,a); printf \"%.2f\",((a[1] * 0.001)+ 1304301600000);}'\n",
    "echo ;\n",
    "zcat part-00009-of-00500.csv.gz | tail -n 1 | awk -F \"\\\"*,\\\"*\" '{ORS=\"\\t\"} {split($0,a); printf \"%.2f\",((a[1] * 0.001)+ 1304301600000);}'\n",
    "echo ;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "f=open (\"Documents/Logs/logs3\",\"w\")\n",
    "f.write(\"FECHA INICIO\\n\")\n",
    "f.write(datetime.fromtimestamp(float(1304317234546.29)/1000).strftime('%Y-%m-%d %H:%M:%S.%f'))\n",
    "f.write(\"\\n\")\n",
    "f.write(datetime.fromtimestamp(float(1304332267297.97)/1000).strftime('%Y-%m-%d %H:%M:%S.%f'))\n",
    "f.write(\"\\n\")\n",
    "f.write(datetime.fromtimestamp(float(1304347300877.30)/1000).strftime('%Y-%m-%d %H:%M:%S.%f'))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "f=open (\"Documents/Logs/logs4\",\"w\")\n",
    "f.write(\"FECHA FIN\\n\")\n",
    "f.write(datetime.fromtimestamp(float(1304322244631.19)/1000).strftime('%Y-%m-%d %H:%M:%S.%f'))\n",
    "f.write(\"\\n\")\n",
    "f.write(datetime.fromtimestamp(float(1304337277640.49)/1000).strftime('%Y-%m-%d %H:%M:%S.%f'))\n",
    "f.write(\"\\n\")\n",
    "f.write(datetime.fromtimestamp(float(1304352311946.04)/1000).strftime('%Y-%m-%d %H:%M:%S.%f'))\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "cd Documents\n",
    "\n",
    "\n",
    "if [ -f Logs/logs5 ]; then\n",
    "    rm Logs/logs5\n",
    "fi\n",
    "\n",
    "echo \"EVENTOS\" >> Logs/logs5\n",
    "\n",
    "for i in $(ls -AS SistemasOperativos/); do\n",
    "\n",
    "    lines=$(zcat SistemasOperativos/\"$i\" | wc -l)\n",
    "\n",
    "    echo \"$lines\" >> Logs/logs5\n",
    "\n",
    "done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "cd Documents/Logs\n",
    "\n",
    "paste logs1 logs2 logs5 logs3 logs4 > LogFinal.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jonathan\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
